{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can an LLM Be Your Recommendation Engine?\n",
    "\n",
    "**A Reproducible Experiment in Cold-Start Fashion Recommendation**\n",
    "\n",
    "This notebook walks through a complete experiment testing whether an LLM can infer a user's latent style preferences from just 10 click signals and make accurate cross-retailer recommendations — no collaborative filtering, no purchase history, no style quiz.\n",
    "\n",
    "## Experiment Overview\n",
    "\n",
    "| Step | What happens | Key output |\n",
    "|------|-------------|------------|\n",
    "| **Phase 1** | Collect training data: 10 items the user clicked + 35 items they saw but skipped | `training_data.json`, `negative_samples.json` |\n",
    "| **Phase 2A** | Synthesize a preference profile from positive signals only (5-agent ensemble) | `phase2a_preference_brief.md` |\n",
    "| **Phase 2B** | Synthesize a profile from positive + negative signals (5-agent ensemble) | `phase3_preference_brief.md` (contrastive) |\n",
    "| **Phase 3** | Score 103 new items from 2 unseen retailers against both profiles | `fp_scores.json`, `br_scores.json` (and `_2a` variants) |\n",
    "| **Phase 4** | Blind evaluation: user labels all 103 items without seeing scores | `phase3_full_results.json` |\n",
    "| **Phase 5** | Compute precision, recall, lift, tier calibration | Analysis below |\n",
    "\n",
    "## What You Need to Reproduce This\n",
    "\n",
    "1. **An Anthropic API key** (set as `ANTHROPIC_API_KEY` environment variable)\n",
    "2. **Your own training data**: 5-15 items you'd click on + items you'd skip from a product category page\n",
    "3. **A test catalog**: Items from a *different* retailer to score against the learned profile\n",
    "4. **~$5-25** in API credits (depending on model choice and catalog size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# !pip install anthropic pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import json\n",
    "import base64\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "\n",
    "client = anthropic.Anthropic()  # reads ANTHROPIC_API_KEY from env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Configuration ────────────────────────────────────────────────────\n# Change these to customize the experiment for your own data.\n\nMODEL = \"claude-sonnet-4-5-20250929\"  # or \"claude-opus-4-6\", \"claude-haiku-4-5-20251001\"\nNUM_AGENTS = 5                         # how many independent synthesis runs to ensemble\nDATA_DIR = Path(\"data\")                # packaged experiment data\n\n# Tier definitions (used for scoring)\nTIER_SCHEMA = {\n    1: \"Strong Match — high confidence the user would click to explore this item\",\n    2: \"Moderate Match — aligns with several preferences but has notable gaps\",\n    3: \"Weak Match — one or two alignment points, but overall not a fit\",\n    4: \"No Match — conflicts with core preferences or hits anti-preferences\",\n}\n\nprint(f\"Model: {MODEL}\")\nprint(f\"Ensemble size: {NUM_AGENTS} agents\")\nprint(f\"Data dir: {DATA_DIR.resolve()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1: Training Data\n",
    "\n",
    "The raw signal is minimal: a user browsed ~370 jackets & coats on Anthropologie.com and **clicked on 10** (positive signal). We also captured **35 items the user saw but did not click** (negative signal).\n",
    "\n",
    "Each item includes:\n",
    "- Name, price, brand\n",
    "- 1-2 hero images (what the user saw on the category browse page)\n",
    "- Product attributes (material, silhouette, color, etc.)\n",
    "\n",
    "### Collecting your own training data\n",
    "\n",
    "To replicate with your own preferences:\n",
    "1. Browse a product category page on any retailer\n",
    "2. Save 5-15 items you'd click to explore further → `training_data.json`\n",
    "3. Save 20-50 items you saw but skipped → `negative_samples.json`\n",
    "4. For each item, save: name, price, and at least 1 hero image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the training data from the original experiment\nwith open(DATA_DIR / \"training\" / \"training_data.json\") as f:\n    training_data = json.load(f)\n\nwith open(DATA_DIR / \"training\" / \"negative_samples.json\") as f:\n    negative_samples = json.load(f)\n\npositive_items = training_data[\"items\"]\nnegative_items = negative_samples[\"items\"] if isinstance(negative_samples, dict) else negative_samples\n\nprint(f\"Positive items (clicked): {len(positive_items)}\")\nprint(f\"Negative items (skipped): {len(negative_items)}\")\nprint(f\"Source: {training_data.get('source', 'N/A')}\")\nprint(f\"Category: {training_data.get('category', 'N/A')}\")\nprint()\nprint(\"Positive items:\")\nfor item in positive_items:\n    print(f\"  {item['id']:>3}. {item['name']:<55} ${item['price_usd']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper: Encode images for the API\n",
    "\n",
    "The Anthropic API accepts images as base64-encoded content blocks. This helper loads a local image and formats it for the messages API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mimetypes\n",
    "\n",
    "def image_to_content_block(image_path: str) -> dict:\n",
    "    \"\"\"Convert a local image file to an Anthropic API content block.\"\"\"\n",
    "    path = Path(image_path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Image not found: {path}\")\n",
    "    mime, _ = mimetypes.guess_type(str(path))\n",
    "    if not mime:\n",
    "        mime = \"image/jpeg\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode(\"ascii\")\n",
    "    return {\n",
    "        \"type\": \"image\",\n",
    "        \"source\": {\"type\": \"base64\", \"media_type\": mime, \"data\": b64},\n",
    "    }\n",
    "\n",
    "\n",
    "def build_item_content_blocks(item: dict, include_images: bool = True) -> list:\n",
    "    \"\"\"Build a list of content blocks (text + images) for one item.\"\"\"\n",
    "    blocks = []\n",
    "    text_desc = f\"Item {item.get('id', '?')}: {item['name']} — ${item.get('price_usd', item.get('price', '?'))}\"\n",
    "    if \"visual_description\" in item:\n",
    "        text_desc += f\"\\nVisual: {item['visual_description']}\"\n",
    "    if \"key_details\" in item:\n",
    "        text_desc += f\"\\nDetails: {item['key_details']}\"\n",
    "    blocks.append({\"type\": \"text\", \"text\": text_desc})\n",
    "\n",
    "    if include_images:\n",
    "        for img_key in [\"hero_img_path\", \"hero_img2_path\"]:\n",
    "            if img_key in item and item[img_key] and Path(item[img_key]).exists():\n",
    "                blocks.append(image_to_content_block(item[img_key]))\n",
    "    return blocks\n",
    "\n",
    "\n",
    "# Quick test\n",
    "sample = positive_items[0]\n",
    "blocks = build_item_content_blocks(sample)\n",
    "print(f\"Built {len(blocks)} content blocks for: {sample['name']}\")\n",
    "print(f\"  Block types: {[b['type'] for b in blocks]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2A: Positive-Only Preference Synthesis\n",
    "\n",
    "We ask the LLM to infer the user's latent preferences from **only the 10 clicked items** — no negative signal.\n",
    "\n",
    "To reduce variance, we run **N independent agents** (same prompt, different samples from the model's output distribution) and merge their outputs into a consensus profile.\n",
    "\n",
    "### The Prompt\n",
    "\n",
    "This is the actual prompt used in the experiment. The key design choices:\n",
    "- We provide **hero images** (what the user saw when browsing), not detail images\n",
    "- We ask for **confidence tiers** (strong / medium / speculation)\n",
    "- We ask for **anti-preferences** inferred from absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE_ONLY_SYSTEM_PROMPT = \"\"\"You are a style analyst specializing in inferring latent \n",
    "fashion preferences from implicit behavioral signals. You will receive a set of items \n",
    "that a user clicked on (chose to explore further) while browsing a large product \n",
    "category page. Your task is to infer the user's underlying style preferences.\n",
    "\n",
    "For each preference dimension you identify, classify your confidence:\n",
    "- STRONG PREFERENCES: Consistent patterns across most items (high confidence)\n",
    "- MEDIUM CONFIDENCE: Appears in some items, plausible pattern\n",
    "- ANTI-PREFERENCES: What the user likely avoids (inferred from absence)\n",
    "\n",
    "Be specific and actionable. Instead of \"likes casual styles\", say \"prefers relaxed/\n",
    "oversized bomber silhouettes in cropped lengths with textural complexity.\"\n",
    "\n",
    "Analyze the IMAGES carefully — visual signals (texture, drape, color temperature, \n",
    "styling context) are often more informative than text descriptions.\"\"\"\n",
    "\n",
    "\n",
    "def build_positive_only_prompt(items: list) -> list:\n",
    "    \"\"\"Build the user message content blocks for positive-only synthesis.\"\"\"\n",
    "    content = []\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            f\"A user browsed ~370 jackets & coats on Anthropologie.com and selected \"\n",
    "            f\"these {len(items)} items (clicked to view detail page). \"\n",
    "            f\"Infer the user's implicit style preferences from ONLY these positive signals.\\n\\n\"\n",
    "            f\"For each item below, you'll see the name, price, and the hero images \"\n",
    "            f\"visible on the category browse page (what the user saw before clicking).\\n\"\n",
    "        ),\n",
    "    })\n",
    "    for item in items:\n",
    "        content.extend(build_item_content_blocks(item, include_images=True))\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"\\n\\nBased on these clicked items, produce a comprehensive User Preference Profile \"\n",
    "            \"with: STRONG PREFERENCES, MEDIUM CONFIDENCE PREFERENCES, and ANTI-PREFERENCES \"\n",
    "            \"(inferred from what's absent in the selections). \"\n",
    "            \"Cover: silhouette, material/texture, color palette, closure type, \"\n",
    "            \"embellishment/details, price behavior, and overall aesthetic.\"\n",
    "        ),\n",
    "    })\n",
    "    return content\n",
    "\n",
    "\n",
    "print(\"Prompt builder ready. Preview of text blocks:\")\n",
    "preview = build_positive_only_prompt(positive_items)\n",
    "for block in preview:\n",
    "    if block[\"type\"] == \"text\":\n",
    "        print(f\"  [text] {block['text'][:120]}...\")\n",
    "    else:\n",
    "        print(f\"  [image] {block['source']['media_type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Run the 5-agent ensemble (Phase 2A) ──────────────────────────────\n# Each agent gets the identical prompt but produces an independent synthesis.\n# Temperature > 0 ensures variation across runs.\n#\n# ⚠️  This cell makes API calls. Cost estimate: ~$3-8 depending on model.\n#     Set RUN_API_CALLS = True to execute, or load saved results below.\n\nRUN_API_CALLS = False  # Set to True to actually call the API\n\nphase2a_syntheses = []\n\nif RUN_API_CALLS:\n    user_content = build_positive_only_prompt(positive_items)\n    for i in range(NUM_AGENTS):\n        print(f\"Running agent {i+1}/{NUM_AGENTS}...\")\n        response = client.messages.create(\n            model=MODEL,\n            max_tokens=4096,\n            system=POSITIVE_ONLY_SYSTEM_PROMPT,\n            messages=[{\"role\": \"user\", \"content\": user_content}],\n        )\n        synthesis = response.content[0].text\n        phase2a_syntheses.append(synthesis)\n        print(f\"  Agent {i+1} done — {len(synthesis)} chars\")\n        \n        # Save each agent's output\n        out_dir = DATA_DIR / \"results\" / \"agent_syntheses\"\n        out_dir.mkdir(parents=True, exist_ok=True)\n        with open(out_dir / f\"phase2a_agent{i+1}_synthesis.md\", \"w\") as f:\n            f.write(synthesis)\n    print(f\"\\nAll {NUM_AGENTS} agents complete.\")\nelse:\n    # Load saved results from the original experiment\n    for i in range(1, 6):\n        path = DATA_DIR / \"results\" / \"agent_syntheses\" / f\"phase2a_agent{i}_synthesis.md\"\n        if path.exists():\n            phase2a_syntheses.append(path.read_text())\n    print(f\"Loaded {len(phase2a_syntheses)} saved agent syntheses from the original experiment.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Merge into consensus profile ─────────────────────────────────────\n# We ask the LLM to read all N agent outputs and produce a merged brief,\n# noting which preferences had full consensus vs. partial agreement.\n\nMERGE_PROMPT = \"\"\"You are given {n} independent style analyses of the same user's \nfashion preferences. Each was produced by a separate analyst seeing the same data.\n\nProduce a single merged User Preference Profile that:\n1. Notes consensus level for each preference (e.g., \"5/5 agents agreed\" vs \"3/5\")\n2. Elevates high-consensus findings to STRONG PREFERENCES\n3. Flags disagreements or low-consensus items as SPECULATION\n4. Preserves specific, actionable language (not vague summaries)\n5. Includes ANTI-PREFERENCES with consensus counts\n\nFormat the output as a structured markdown document with clear sections.\"\"\"\n\nif RUN_API_CALLS and phase2a_syntheses:\n    agent_texts = \"\\n\\n---\\n\\n\".join(\n        [f\"## Agent {i+1} Analysis\\n\\n{s}\" for i, s in enumerate(phase2a_syntheses)]\n    )\n    response = client.messages.create(\n        model=MODEL,\n        max_tokens=4096,\n        system=MERGE_PROMPT.format(n=len(phase2a_syntheses)),\n        messages=[{\"role\": \"user\", \"content\": agent_texts}],\n    )\n    phase2a_brief = response.content[0].text\n    with open(DATA_DIR / \"results\" / \"profiles\" / \"positive_only_brief.md\", \"w\") as f:\n        f.write(phase2a_brief)\n    print(\"Merged profile saved.\")\nelse:\n    phase2a_brief = (DATA_DIR / \"results\" / \"profiles\" / \"positive_only_brief.md\").read_text()\n    print(f\"Loaded saved positive-only preference brief ({len(phase2a_brief)} chars).\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"POSITIVE-ONLY PREFERENCE BRIEF (first 2000 chars)\")\nprint(\"=\"*70)\nprint(phase2a_brief[:2000])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2B: Contrastive Preference Synthesis\n",
    "\n",
    "Now we give the LLM **both** signals: the 10 items the user clicked **and** 35 items the user saw but did not click. The hypothesis is that negative signal should refine the profile — turning \"likes bombers\" into \"likes bombers ONLY with embellishment.\"\n",
    "\n",
    "### What changed in the prompt\n",
    "\n",
    "The key addition is asking the LLM to use negative items to:\n",
    "1. **Sharpen boundaries** — what specifically within a liked category gets rejected?\n",
    "2. **Add conditional logic** — \"likes X only when Y\"\n",
    "3. **Confirm anti-preferences** with direct evidence (not just absence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRASTIVE_SYSTEM_PROMPT = \"\"\"You are a style analyst specializing in inferring latent \n",
    "fashion preferences from implicit behavioral signals. You will receive two sets of items:\n",
    "\n",
    "1. POSITIVE items: items the user clicked on (chose to explore further)\n",
    "2. NEGATIVE items: items the user saw on the same page but did NOT click\n",
    "\n",
    "Use both signals to build a nuanced preference profile. The negative items are especially\n",
    "valuable for:\n",
    "- Sharpening boundaries within liked categories (e.g., \"likes leather BUT NOT in moto style\")\n",
    "- Adding conditional logic (\"likes embroidery ONLY in structured silhouettes\")\n",
    "- Confirming anti-preferences with direct evidence\n",
    "\n",
    "For each preference, classify confidence and note whether it comes from positive signal,\n",
    "negative signal, or both.\n",
    "\n",
    "Analyze the IMAGES carefully — visual signals are often more informative than text.\"\"\"\n",
    "\n",
    "\n",
    "def build_contrastive_prompt(pos_items: list, neg_items: list) -> list:\n",
    "    \"\"\"Build user message for contrastive synthesis.\"\"\"\n",
    "    content = []\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            f\"A user browsed ~370 jackets & coats on Anthropologie.com. \"\n",
    "            f\"They clicked on {len(pos_items)} items (POSITIVE signal) and saw but \"\n",
    "            f\"did NOT click on {len(neg_items)} other items (NEGATIVE signal).\\n\\n\"\n",
    "            f\"── POSITIVE ITEMS (user clicked to view) ──\\n\"\n",
    "        ),\n",
    "    })\n",
    "    for item in pos_items:\n",
    "        content.extend(build_item_content_blocks(item, include_images=True))\n",
    "\n",
    "    content.append({\"type\": \"text\", \"text\": \"\\n── NEGATIVE ITEMS (user saw but skipped) ──\\n\"})\n",
    "    for item in neg_items:\n",
    "        content.extend(build_item_content_blocks(item, include_images=True))\n",
    "\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"\\n\\nProduce a User Preference Profile using BOTH positive and negative signals. \"\n",
    "            \"For each preference: state it, note the evidence (which positive items support it, \"\n",
    "            \"which negative items confirm the boundary), and classify confidence.\\n\"\n",
    "            \"Include: STRONG PREFERENCES, CONDITIONAL PREFERENCES (likes X only when Y), \"\n",
    "            \"and ANTI-PREFERENCES (with specific negative-item evidence).\"\n",
    "        ),\n",
    "    })\n",
    "    return content\n",
    "\n",
    "\n",
    "print(f\"Contrastive prompt will include {len(positive_items)} positive + {len(negative_items)} negative items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Run 5-agent ensemble for contrastive synthesis ────────────────────\n# ⚠️  This cell is more expensive than 2A because of the larger input (45 items with images).\n#     Cost estimate: ~$8-15 depending on model.\n\nphase2b_syntheses = []\n\nif RUN_API_CALLS:\n    user_content = build_contrastive_prompt(positive_items, negative_items)\n    for i in range(NUM_AGENTS):\n        print(f\"Running agent {i+1}/{NUM_AGENTS}...\")\n        response = client.messages.create(\n            model=MODEL,\n            max_tokens=6000,\n            system=CONTRASTIVE_SYSTEM_PROMPT,\n            messages=[{\"role\": \"user\", \"content\": user_content}],\n        )\n        synthesis = response.content[0].text\n        phase2b_syntheses.append(synthesis)\n        print(f\"  Agent {i+1} done — {len(synthesis)} chars\")\n    print(f\"\\nAll {NUM_AGENTS} agents complete.\")\nelse:\n    for i in range(1, 6):\n        path = DATA_DIR / \"results\" / \"agent_syntheses\" / f\"phase2b_agent{i}_synthesis.md\"\n        if path.exists():\n            phase2b_syntheses.append(path.read_text())\n    print(f\"Loaded {len(phase2b_syntheses)} saved contrastive agent syntheses.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Merge contrastive profile ────────────────────────────────────────\n\nif RUN_API_CALLS and phase2b_syntheses:\n    agent_texts = \"\\n\\n---\\n\\n\".join(\n        [f\"## Agent {i+1} Analysis\\n\\n{s}\" for i, s in enumerate(phase2b_syntheses)]\n    )\n    response = client.messages.create(\n        model=MODEL,\n        max_tokens=6000,\n        system=MERGE_PROMPT.format(n=len(phase2b_syntheses)),\n        messages=[{\"role\": \"user\", \"content\": agent_texts}],\n    )\n    phase2b_brief = response.content[0].text\n    with open(DATA_DIR / \"results\" / \"profiles\" / \"contrastive_brief.md\", \"w\") as f:\n        f.write(phase2b_brief)\n    print(\"Merged contrastive profile saved.\")\nelse:\n    phase2b_brief = (DATA_DIR / \"results\" / \"profiles\" / \"contrastive_brief.md\").read_text()\n    print(f\"Loaded saved contrastive preference brief ({len(phase2b_brief)} chars).\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"CONTRASTIVE PREFERENCE BRIEF (first 2000 chars)\")\nprint(\"=\"*70)\nprint(phase2b_brief[:2000])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What did negative signal add?\n",
    "\n",
    "The contrastive profile's main value-add is **conditional logic** — turning flat preferences into boundary-aware rules:\n",
    "\n",
    "| Positive-only said | Contrastive refined to |\n",
    "|---|---|\n",
    "| \"Likes bombers\" | \"Likes bombers ONLY with embellishment\" (skipped plain bombers at $360-400) |\n",
    "| \"Likes faux leather\" | \"Likes faux leather in unconventional silhouettes only\" (skipped standard moto) |\n",
    "| \"Likes craft details\" | \"Likes craft details in structured silhouettes only\" (skipped boho/tie-front with same details) |\n",
    "| \"Prefers relaxed fit\" | \"Actively rejects fitted/body-conscious\" (confirmed by 3+ skipped fitted items) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 3: Score New Catalog Items\n",
    "\n",
    "Now we test the profiles on **unseen items from different retailers**. This is the cold-start cross-retailer transfer test.\n",
    "\n",
    "- **Free People**: 70 jackets & coats\n",
    "- **Banana Republic**: 33 jackets & coats\n",
    "\n",
    "Each item is scored on the 4-tier scale by both the positive-only and contrastive profiles.\n",
    "\n",
    "### The Scoring Prompt\n",
    "\n",
    "For each item, we send the preference brief + the item's image and metadata, and ask the LLM to assign a tier with a rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORING_SYSTEM_PROMPT = \"\"\"You are a recommendation engine. You have a detailed user \n",
    "preference profile and must score new items on how well they match.\n",
    "\n",
    "Score each item on this 4-tier scale:\n",
    "- Tier 1 (Strong Match): High confidence the user would click to explore this item\n",
    "- Tier 2 (Moderate Match): Aligns with several preferences but has notable gaps  \n",
    "- Tier 3 (Weak Match): One or two alignment points, but overall not a fit\n",
    "- Tier 4 (No Match): Conflicts with core preferences or hits anti-preferences\n",
    "\n",
    "For each item, provide:\n",
    "1. The tier (1-4)\n",
    "2. A 1-2 sentence rationale citing specific preference dimensions\n",
    "\n",
    "Be calibrated: Tier 1 should be rare (~5-10% of items). Most items should be Tier 3-4.\n",
    "Analyze the IMAGE carefully — visual match matters more than text description match.\"\"\"\n",
    "\n",
    "\n",
    "def score_single_item(item: dict, preference_brief: str, item_image_path: str = None) -> dict:\n",
    "    \"\"\"Score a single catalog item against a preference profile.\"\"\"\n",
    "    content = []\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": f\"## User Preference Profile\\n\\n{preference_brief}\",\n",
    "    })\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": f\"\\n## Item to Score\\n\\nName: {item['name']}\\nPrice: {item.get('price', 'N/A')}\",\n",
    "    })\n",
    "    if item_image_path and Path(item_image_path).exists():\n",
    "        content.append(image_to_content_block(item_image_path))\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"\\nScore this item. Respond in this exact format:\\nTier: [1-4]\\nRationale: [your reasoning]\",\n",
    "    })\n",
    "\n",
    "    response = client.messages.create(\n",
    "        model=MODEL,\n",
    "        max_tokens=300,\n",
    "        system=SCORING_SYSTEM_PROMPT,\n",
    "        messages=[{\"role\": \"user\", \"content\": content}],\n",
    "    )\n",
    "    text = response.content[0].text\n",
    "\n",
    "    # Parse tier from response\n",
    "    tier_match = re.search(r\"Tier:\\s*(\\d)\", text)\n",
    "    tier = int(tier_match.group(1)) if tier_match else None\n",
    "    rationale_match = re.search(r\"Rationale:\\s*(.+)\", text, re.DOTALL)\n",
    "    rationale = rationale_match.group(1).strip() if rationale_match else text\n",
    "\n",
    "    return {\"id\": item[\"id\"], \"name\": item[\"name\"], \"tier\": tier, \"rationale\": rationale}\n",
    "\n",
    "\n",
    "def score_catalog(catalog: list, preference_brief: str, image_dir: str = None) -> list:\n",
    "    \"\"\"Score an entire catalog of items. Returns list of scored items.\"\"\"\n",
    "    results = []\n",
    "    for i, item in enumerate(catalog):\n",
    "        img_path = None\n",
    "        if image_dir:\n",
    "            # Try common naming patterns\n",
    "            for ext in [\".jpg\", \".png\", \".webp\"]:\n",
    "                candidate = Path(image_dir) / f\"{item['id']}{ext}\"\n",
    "                if candidate.exists():\n",
    "                    img_path = str(candidate)\n",
    "                    break\n",
    "        if not img_path and \"img_url\" in item:\n",
    "            img_path = item.get(\"img_path\")  # fallback to relative path in data\n",
    "            \n",
    "        result = score_single_item(item, preference_brief, img_path)\n",
    "        results.append(result)\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Scored {i+1}/{len(catalog)} items...\")\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Scoring functions ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Load test catalogs ───────────────────────────────────────────────\n\nwith open(DATA_DIR / \"test\" / \"fp_catalog.json\") as f:\n    fp_catalog = json.load(f)\nwith open(DATA_DIR / \"test\" / \"br_catalog.json\") as f:\n    br_catalog = json.load(f)\n\nprint(f\"Free People catalog: {len(fp_catalog)} items\")\nprint(f\"Banana Republic catalog: {len(br_catalog)} items\")\nprint(f\"Total items to score: {len(fp_catalog) + len(br_catalog)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Score catalogs (or load saved scores) ────────────────────────────\n# ⚠️  Scoring 103 items × 2 profiles = 206 API calls.\n#     Cost estimate: ~$15-20 with Sonnet, ~$2-4 with Haiku.\n\nif RUN_API_CALLS:\n    img_base = str(DATA_DIR / \"test\" / \"images\")\n    print(\"Scoring Free People with contrastive profile...\")\n    fp_scores_contrastive = score_catalog(fp_catalog, phase2b_brief, f\"{img_base}/fp\")\n    print(\"Scoring Banana Republic with contrastive profile...\")\n    br_scores_contrastive = score_catalog(br_catalog, phase2b_brief, f\"{img_base}/br\")\n    print(\"Scoring Free People with positive-only profile...\")\n    fp_scores_posonly = score_catalog(fp_catalog, phase2a_brief, f\"{img_base}/fp\")\n    print(\"Scoring Banana Republic with positive-only profile...\")\n    br_scores_posonly = score_catalog(br_catalog, phase2a_brief, f\"{img_base}/br\")\n    \n    # Save\n    scores_dir = DATA_DIR / \"results\" / \"scores\"\n    scores_dir.mkdir(parents=True, exist_ok=True)\n    for name, data in [(\"fp_scores_contrastive\", fp_scores_contrastive),\n                       (\"br_scores_contrastive\", br_scores_contrastive),\n                       (\"fp_scores_posonly\", fp_scores_posonly),\n                       (\"br_scores_posonly\", br_scores_posonly)]:\n        with open(scores_dir / f\"{name}.json\", \"w\") as f:\n            json.dump(data, f, indent=2)\n    print(\"All scores saved.\")\nelse:\n    # Load saved scores from the original experiment\n    scores_dir = DATA_DIR / \"results\" / \"scores\"\n    with open(scores_dir / \"fp_scores_contrastive.json\") as f:\n        fp_scores_contrastive = json.load(f)\n    with open(scores_dir / \"br_scores_contrastive.json\") as f:\n        br_scores_contrastive = json.load(f)\n    with open(scores_dir / \"fp_scores_posonly.json\") as f:\n        fp_scores_posonly = json.load(f)\n    with open(scores_dir / \"br_scores_posonly.json\") as f:\n        br_scores_posonly = json.load(f)\n    print(\"Loaded saved scores from the original experiment.\")\n\n# Combine into a single DataFrame for analysis\nall_contrastive = fp_scores_contrastive + br_scores_contrastive\nall_posonly = fp_scores_posonly + br_scores_posonly\nprint(f\"\\nContrastive scores: {len(all_contrastive)} items\")\nprint(f\"Positive-only scores: {len(all_posonly)} items\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Tier distribution comparison ─────────────────────────────────────\n",
    "\n",
    "def tier_counts(scores):\n",
    "    c = Counter(s[\"tier\"] for s in scores)\n",
    "    return {t: c.get(t, 0) for t in [1, 2, 3, 4]}\n",
    "\n",
    "ct = tier_counts(all_contrastive)\n",
    "pt = tier_counts(all_posonly)\n",
    "\n",
    "print(\"Tier distribution:\")\n",
    "print(f\"{'Tier':<8} {'Contrastive':>12} {'Positive-only':>14}\")\n",
    "print(\"-\" * 36)\n",
    "for t in [1, 2, 3, 4]:\n",
    "    print(f\"Tier {t:<3} {ct[t]:>12} {pt[t]:>14}\")\n",
    "print(f\"{'Total':<8} {sum(ct.values()):>12} {sum(pt.values()):>14}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "for ax, (title, counts) in zip(axes, [(\"Contrastive Profile\", ct), (\"Positive-Only Profile\", pt)]):\n",
    "    colors = [\"#5C7D68\", \"#B8726A\", \"#D8CFBF\", \"#A0908A\"]\n",
    "    ax.bar([f\"Tier {t}\" for t in [1,2,3,4]], [counts[t] for t in [1,2,3,4]], color=colors)\n",
    "    ax.set_title(title, fontsize=13)\n",
    "    ax.set_ylabel(\"Number of items\")\n",
    "    for i, t in enumerate([1,2,3,4]):\n",
    "        ax.text(i, counts[t] + 0.5, str(counts[t]), ha=\"center\", fontsize=11)\n",
    "plt.suptitle(\"How the LLM distributed items across tiers\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 4: Blind Evaluation\n",
    "\n",
    "The user evaluated all 103 items **blind** — items were shuffled, and no tier labels or retailer names were shown. For each item, the user saw two hero images and indicated whether they'd click to explore further (binary: click / skip).\n",
    "\n",
    "### Generating a blind evaluation UI\n",
    "\n",
    "The cell below creates a shuffled HTML file that a user can open in a browser to do the blind evaluation. Each item shows two images and a click/skip button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_blind_eval_html(scored_items: list, output_path: str, image_base: str = \".\"):\n",
    "    \"\"\"Generate a shuffled HTML evaluation page for blind labeling.\n",
    "    \n",
    "    The user opens this in a browser, clicks 'Would Explore' or 'Skip' for each item,\n",
    "    then copies the JSON results at the end.\n",
    "    \"\"\"\n",
    "    # Shuffle items (but keep a key to map back)\n",
    "    shuffled = list(enumerate(scored_items))\n",
    "    random.shuffle(shuffled)\n",
    "    \n",
    "    html = \"\"\"<!DOCTYPE html>\n",
    "<html><head><meta charset=\"utf-8\"><title>Blind Evaluation</title>\n",
    "<style>\n",
    "  body { font-family: system-ui; max-width: 800px; margin: 40px auto; background: #f5f5f5; }\n",
    "  .item { background: white; padding: 20px; margin: 16px 0; border-radius: 12px;\n",
    "          box-shadow: 0 1px 3px rgba(0,0,0,0.1); text-align: center; }\n",
    "  .item img { max-width: 300px; max-height: 400px; margin: 8px; border-radius: 8px; }\n",
    "  .buttons { margin-top: 12px; }\n",
    "  .buttons button { padding: 10px 24px; margin: 0 8px; border: none; border-radius: 8px;\n",
    "                    cursor: pointer; font-size: 14px; }\n",
    "  .btn-click { background: #5C7D68; color: white; }\n",
    "  .btn-skip { background: #D8CFBF; color: #2A2320; }\n",
    "  .done { opacity: 0.4; }\n",
    "  #results { margin-top: 40px; background: white; padding: 20px; border-radius: 12px;\n",
    "             white-space: pre-wrap; font-family: monospace; font-size: 12px; }\n",
    "</style>\n",
    "</head><body>\n",
    "<h1>Blind Item Evaluation</h1>\n",
    "<p>For each item, decide: would you click to explore this further?</p>\n",
    "<p id=\"progress\">0 / \"\"\" + str(len(scored_items)) + \"\"\" evaluated</p>\\n\"\"\"\n",
    "\n",
    "    for display_pos, (orig_idx, item) in enumerate(shuffled):\n",
    "        img_path = item.get(\"img_path\", \"\")\n",
    "        html += f\"\"\"<div class=\"item\" id=\"item-{display_pos}\">\n",
    "  <div style=\"color:#999;font-size:12px\">Item {display_pos + 1} of {len(scored_items)}</div>\n",
    "  <div><img src=\"{image_base}/{img_path}\" onerror=\"this.style.display='none'\"></div>\n",
    "  <div class=\"buttons\">\n",
    "    <button class=\"btn-click\" onclick=\"record('{item['id']}', true, this)\">Would Explore</button>\n",
    "    <button class=\"btn-skip\" onclick=\"record('{item['id']}', false, this)\">Skip</button>\n",
    "  </div>\n",
    "</div>\\n\"\"\"\n",
    "\n",
    "    html += \"\"\"\n",
    "<div id=\"results\" style=\"display:none\">\n",
    "  <h3>Results (copy this JSON):</h3>\n",
    "  <pre id=\"results-json\"></pre>\n",
    "</div>\n",
    "<script>\n",
    "const results = {};\n",
    "let count = 0;\n",
    "const total = \"\"\" + str(len(scored_items)) + \"\"\";\n",
    "function record(id, clicked, btn) {\n",
    "  if (results[id] !== undefined) return;\n",
    "  results[id] = clicked;\n",
    "  btn.closest('.item').classList.add('done');\n",
    "  count++;\n",
    "  document.getElementById('progress').textContent = count + ' / ' + total + ' evaluated';\n",
    "  if (count === total) {\n",
    "    document.getElementById('results').style.display = 'block';\n",
    "    document.getElementById('results-json').textContent = JSON.stringify(results, null, 2);\n",
    "  }\n",
    "}\n",
    "</script>\n",
    "</body></html>\"\"\"\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(html)\n",
    "    print(f\"Blind evaluation HTML written to: {output_path}\")\n",
    "    print(f\"Items: {len(scored_items)} (shuffled)\")\n",
    "    \n",
    "    # Save the shuffle key for later mapping\n",
    "    shuffle_key = [{\"display_pos\": dp, \"id\": item[\"id\"]} for dp, (_, item) in enumerate(shuffled)]\n",
    "    key_path = output_path.replace(\".html\", \"_shuffle_key.json\")\n",
    "    with open(key_path, \"w\") as f:\n",
    "        json.dump(shuffle_key, f, indent=2)\n",
    "    print(f\"Shuffle key saved to: {key_path}\")\n",
    "\n",
    "\n",
    "# Example: generate eval HTML (uncomment to run)\n",
    "# generate_blind_eval_html(fp_scores_contrastive + br_scores_contrastive, \"blind_eval.html\")\n",
    "print(\"Blind evaluation generator ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Load ground truth from the original experiment ────────────────────\n\nwith open(DATA_DIR / \"results\" / \"evaluation\" / \"phase3_full_results.json\") as f:\n    full_results = json.load(f)\n\n# Build a set of clicked item IDs from the ground truth\nclicked_ids = {item[\"id\"] for item in full_results[\"clicked_items\"]}\n\nprint(f\"Total items evaluated: {full_results['total_items']}\")\nprint(f\"Total clicks (user would explore): {full_results['total_clicks']}\")\nprint(f\"Overall click rate: {full_results['overall_click_rate']}\")\nprint(f\"\\nClicked items: {sorted(clicked_ids)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 5: Analysis\n",
    "\n",
    "Now we merge the LLM's tier scores with the user's blind click labels and compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Build analysis DataFrame ─────────────────────────────────────────\n",
    "\n",
    "rows = []\n",
    "for item in all_contrastive:\n",
    "    item_id = item[\"id\"]\n",
    "    # Find the positive-only score for the same item\n",
    "    posonly_match = next((s for s in all_posonly if s[\"id\"] == item_id), None)\n",
    "    rows.append({\n",
    "        \"id\": item_id,\n",
    "        \"name\": item[\"name\"],\n",
    "        \"retailer\": \"Free People\" if item_id.startswith(\"FP\") else \"Banana Republic\",\n",
    "        \"tier_contrastive\": item[\"tier\"],\n",
    "        \"tier_posonly\": posonly_match[\"tier\"] if posonly_match else None,\n",
    "        \"user_clicked\": item_id in clicked_ids,\n",
    "        \"rationale_contrastive\": item.get(\"rationale\", \"\"),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df[\"is_rec_contrastive\"] = df[\"tier_contrastive\"].isin([1, 2])\n",
    "df[\"is_rec_posonly\"] = df[\"tier_posonly\"].isin([1, 2])\n",
    "\n",
    "print(f\"Analysis DataFrame: {len(df)} items\")\n",
    "print(f\"\\nRetailer breakdown:\")\n",
    "print(df[\"retailer\"].value_counts().to_string())\n",
    "print(f\"\\nUser click rate: {df['user_clicked'].mean():.1%}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Tier calibration ─────────────────────────────────────────────────\n",
    "# For each tier: what fraction did the user actually click?\n",
    "# A well-calibrated system has monotonically decreasing click rates from T1→T4.\n",
    "\n",
    "def tier_calibration(df, tier_col, label=\"\"):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Tier Calibration: {label}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    cal = df.groupby(tier_col).agg(\n",
    "        n_items=(\"user_clicked\", \"count\"),\n",
    "        n_clicked=(\"user_clicked\", \"sum\"),\n",
    "    )\n",
    "    cal[\"click_rate\"] = cal[\"n_clicked\"] / cal[\"n_items\"]\n",
    "    cal.index.name = \"Tier\"\n",
    "    print(cal.to_string(formatters={\"click_rate\": \"{:.1%}\".format}))\n",
    "    return cal\n",
    "\n",
    "cal_ct = tier_calibration(df, \"tier_contrastive\", \"Contrastive Profile\")\n",
    "cal_po = tier_calibration(df, \"tier_posonly\", \"Positive-Only Profile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Precision, Recall, Lift ──────────────────────────────────────────\n",
    "\n",
    "def compute_metrics(df, rec_col, label=\"\"):\n",
    "    recs = df[df[rec_col]]\n",
    "    non_recs = df[~df[rec_col]]\n",
    "    \n",
    "    tp = recs[\"user_clicked\"].sum()\n",
    "    fp = len(recs) - tp\n",
    "    fn = non_recs[\"user_clicked\"].sum()\n",
    "    tn = len(non_recs) - fn\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    base_rate = df[\"user_clicked\"].mean()\n",
    "    lift = precision / base_rate if base_rate > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Metrics: {label}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Recommendations (T1+T2): {len(recs)} items\")\n",
    "    print(f\"Non-recommendations (T3+T4): {len(non_recs)} items\")\n",
    "    print(f\"\")\n",
    "    print(f\"True Positives:  {tp:>3}  (recommended & user clicked)\")\n",
    "    print(f\"False Positives: {fp:>3}  (recommended but user skipped)\")\n",
    "    print(f\"False Negatives: {fn:>3}  (not recommended but user clicked)\")\n",
    "    print(f\"True Negatives:  {tn:>3}  (not recommended & user skipped)\")\n",
    "    print(f\"\")\n",
    "    print(f\"Precision: {precision:.1%}  (of items we recommended, how many did the user like?)\")\n",
    "    print(f\"Recall:    {recall:.1%}  (of items the user liked, how many did we recommend?)\")\n",
    "    print(f\"Lift:      {lift:.2f}x  (vs. {base_rate:.1%} base click rate)\")\n",
    "    \n",
    "    return {\"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn,\n",
    "            \"precision\": precision, \"recall\": recall, \"lift\": lift}\n",
    "\n",
    "m_ct = compute_metrics(df, \"is_rec_contrastive\", \"Contrastive Profile (T1+T2 = recommendation)\")\n",
    "m_po = compute_metrics(df, \"is_rec_posonly\", \"Positive-Only Profile (T1+T2 = recommendation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Side-by-side comparison ──────────────────────────────────────────\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Contrastive\": m_ct,\n",
    "    \"Positive-Only\": m_po,\n",
    "}).T\n",
    "\n",
    "comparison[\"precision\"] = comparison[\"precision\"].map(\"{:.1%}\".format)\n",
    "comparison[\"recall\"] = comparison[\"recall\"].map(\"{:.1%}\".format)\n",
    "comparison[\"lift\"] = comparison[\"lift\"].map(\"{:.2f}x\".format)\n",
    "comparison[[\"tp\", \"fp\", \"fn\", \"tn\"]] = comparison[[\"tp\", \"fp\", \"fn\", \"tn\"]].astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HEAD-TO-HEAD: Contrastive vs Positive-Only\")\n",
    "print(\"=\"*60)\n",
    "print(comparison.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Visualization: Tier calibration curves ───────────────────────────\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "tiers = [1, 2, 3, 4]\n",
    "ct_rates = [cal_ct.loc[t, \"click_rate\"] if t in cal_ct.index else 0 for t in tiers]\n",
    "po_rates = [cal_po.loc[t, \"click_rate\"] if t in cal_po.index else 0 for t in tiers]\n",
    "\n",
    "x = range(len(tiers))\n",
    "width = 0.35\n",
    "bars1 = ax.bar([i - width/2 for i in x], ct_rates, width, label=\"Contrastive\", color=\"#B8726A\")\n",
    "bars2 = ax.bar([i + width/2 for i in x], po_rates, width, label=\"Positive-Only\", color=\"#5C7D68\")\n",
    "\n",
    "ax.axhline(y=df[\"user_clicked\"].mean(), color=\"gray\", linestyle=\"--\", alpha=0.7, label=f\"Base rate ({df['user_clicked'].mean():.1%})\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"Tier {t}\" for t in tiers])\n",
    "ax.set_ylabel(\"User click rate\")\n",
    "ax.set_title(\"Tier Calibration: Did higher tiers actually get more clicks?\")\n",
    "ax.yaxis.set_major_formatter(mticker.PercentFormatter(1.0))\n",
    "ax.legend()\n",
    "\n",
    "for bar_group in [bars1, bars2]:\n",
    "    for bar in bar_group:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f\"{height:.0%}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Confusion matrix heatmap ─────────────────────────────────────────\n",
    "# Cross-tabulate contrastive tier vs positive-only tier, colored by user click rate\n",
    "\n",
    "cross = df.groupby([\"tier_contrastive\", \"tier_posonly\"]).agg(\n",
    "    count=(\"user_clicked\", \"count\"),\n",
    "    clicks=(\"user_clicked\", \"sum\"),\n",
    ").reset_index()\n",
    "cross[\"click_rate\"] = cross[\"clicks\"] / cross[\"count\"]\n",
    "\n",
    "pivot = cross.pivot(index=\"tier_contrastive\", columns=\"tier_posonly\", values=\"count\").fillna(0).astype(int)\n",
    "pivot_rate = cross.pivot(index=\"tier_contrastive\", columns=\"tier_posonly\", values=\"click_rate\").fillna(0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(pivot, annot=True, fmt=\"d\", cmap=\"YlOrBr\", ax=ax, cbar_kws={\"label\": \"Item count\"})\n",
    "ax.set_xlabel(\"Positive-Only Tier\")\n",
    "ax.set_ylabel(\"Contrastive Tier\")\n",
    "ax.set_title(\"Where do the two profiles agree/disagree?\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Agreement rate\n",
    "agree = (df[\"tier_contrastive\"] == df[\"tier_posonly\"]).mean()\n",
    "print(f\"\\nExact tier agreement: {agree:.1%}\")\n",
    "within_one = (abs(df[\"tier_contrastive\"] - df[\"tier_posonly\"]) <= 1).mean()\n",
    "print(f\"Within 1 tier: {within_one:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Error analysis: False Positives and False Negatives ──────────────\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FALSE POSITIVES (Contrastive): Recommended but user skipped\")\n",
    "print(\"=\"*60)\n",
    "fps = df[(df[\"is_rec_contrastive\"]) & (~df[\"user_clicked\"])]\n",
    "for _, row in fps.iterrows():\n",
    "    print(f\"  {row['id']}: {row['name']} (Tier {row['tier_contrastive']})\")\n",
    "    print(f\"    Rationale: {row['rationale_contrastive'][:120]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FALSE NEGATIVES (Contrastive): User clicked but we didn't recommend\")\n",
    "print(\"=\"*60)\n",
    "fns = df[(~df[\"is_rec_contrastive\"]) & (df[\"user_clicked\"])]\n",
    "for _, row in fns.iterrows():\n",
    "    print(f\"  {row['id']}: {row['name']} (Tier {row['tier_contrastive']})\")\n",
    "    print(f\"    Rationale: {row['rationale_contrastive'][:120]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cost Analysis\n",
    "\n",
    "How much does this cost in practice? The answer depends heavily on model choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cost estimation ──────────────────────────────────────────────────\n",
    "# These are approximate costs based on the original experiment's token usage.\n",
    "\n",
    "PRICING = {\n",
    "    \"claude-opus-4-6\":           {\"input\": 15.0, \"output\": 75.0, \"cache_read\": 1.50},\n",
    "    \"claude-sonnet-4-5-20250929\": {\"input\": 3.0,  \"output\": 15.0, \"cache_read\": 0.30},\n",
    "    \"claude-haiku-4-5-20251001\":  {\"input\": 0.80, \"output\": 4.0,  \"cache_read\": 0.08},\n",
    "}\n",
    "\n",
    "# Estimated tokens per operation (from original experiment)\n",
    "TOKENS_PER_OP = {\n",
    "    \"profile_synthesis_per_agent\": {\"input\": 50_000, \"output\": 3_000},  # with images\n",
    "    \"profile_merge\":               {\"input\": 20_000, \"output\": 2_000},\n",
    "    \"score_single_item\":           {\"input\": 5_000,  \"output\": 200},\n",
    "}\n",
    "\n",
    "def estimate_cost(model: str, n_agents: int, n_items: int, n_profiles: int = 2):\n",
    "    \"\"\"Estimate API cost for a full experiment run.\"\"\"\n",
    "    p = PRICING[model]\n",
    "    \n",
    "    # Profile synthesis: n_agents × n_profiles\n",
    "    synth = TOKENS_PER_OP[\"profile_synthesis_per_agent\"]\n",
    "    synth_cost = n_agents * n_profiles * (\n",
    "        synth[\"input\"] * p[\"input\"] / 1e6 + synth[\"output\"] * p[\"output\"] / 1e6\n",
    "    )\n",
    "    \n",
    "    # Merge: n_profiles\n",
    "    merge = TOKENS_PER_OP[\"profile_merge\"]\n",
    "    merge_cost = n_profiles * (\n",
    "        merge[\"input\"] * p[\"input\"] / 1e6 + merge[\"output\"] * p[\"output\"] / 1e6\n",
    "    )\n",
    "    \n",
    "    # Scoring: n_items × n_profiles\n",
    "    score = TOKENS_PER_OP[\"score_single_item\"]\n",
    "    score_cost = n_items * n_profiles * (\n",
    "        score[\"input\"] * p[\"input\"] / 1e6 + score[\"output\"] * p[\"output\"] / 1e6\n",
    "    )\n",
    "    \n",
    "    total = synth_cost + merge_cost + score_cost\n",
    "    return {\n",
    "        \"synthesis\": synth_cost, \"merge\": merge_cost,\n",
    "        \"scoring\": score_cost, \"total\": total,\n",
    "    }\n",
    "\n",
    "print(f\"{'Model':<35} {'Synthesis':>10} {'Scoring':>10} {'Total':>10}\")\n",
    "print(\"-\" * 67)\n",
    "for model_id in PRICING:\n",
    "    short = model_id.split(\"-\")[1].title()\n",
    "    costs = estimate_cost(model_id, n_agents=5, n_items=103)\n",
    "    print(f\"{short + ' (5 agents, 103 items)':<35} ${costs['synthesis']:>8.2f} ${costs['scoring']:>8.2f} ${costs['total']:>8.2f}\")\n",
    "\n",
    "print(\"\\n--- Smaller experiment (3 agents, 50 items) ---\")\n",
    "for model_id in PRICING:\n",
    "    short = model_id.split(\"-\")[1].title()\n",
    "    costs = estimate_cost(model_id, n_agents=3, n_items=50)\n",
    "    print(f\"{short + ' (3 agents, 50 items)':<35} ${costs['synthesis']:>8.2f} ${costs['scoring']:>8.2f} ${costs['total']:>8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Findings\n",
    "\n",
    "From the original experiment:\n",
    "\n",
    "1. **The LLM understood preferences well from just 10 clicks.** Both profiles correctly identified the core aesthetic (craft-forward bombers, textural complexity, dark earth tones).\n",
    "\n",
    "2. **Negative signal improved understanding but hurt recommendations.** The contrastive profile had sharper conditional logic (\"bombers only with embellishment\") but converted soft preferences into hard rejection gates, causing more false negatives.\n",
    "\n",
    "3. **Positive-only achieved better recall at similar precision.** The simpler profile was more permissive, capturing more items the user actually liked.\n",
    "\n",
    "4. **The \"text bottleneck\" limits recommendation quality.** The image → text → LLM pipeline loses continuous visual information (texture, drape, color temperature) when compressing to discrete text tokens. A native multimodal embedding approach might do better.\n",
    "\n",
    "5. **Tier calibration worked directionally but not perfectly.** T4 items had the lowest click rate (~9%), confirming the system correctly identifies non-matches. But T1-T3 were not as cleanly separated as expected.\n",
    "\n",
    "6. **Cross-retailer transfer is inherently hard.** The user's click rate on Banana Republic (3%) was far below Free People (24%), suggesting brand/aesthetic fit matters beyond individual item attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## How to Run This With Your Own Data\n",
    "\n",
    "### Step 1: Collect training data\n",
    "Browse a product category page and save your clicks and skips:\n",
    "\n",
    "```json\n",
    "// training_data.json\n",
    "{\n",
    "  \"source\": \"your-retailer.com\",\n",
    "  \"category\": \"Jackets & Coats\",\n",
    "  \"items\": [\n",
    "    {\n",
    "      \"id\": 1,\n",
    "      \"name\": \"Item Name\",\n",
    "      \"price_usd\": 150,\n",
    "      \"hero_img_path\": \"images/item1.jpg\",\n",
    "      \"visual_description\": \"optional text description\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Step 2: Set `RUN_API_CALLS = True` and run cells\n",
    "The notebook will synthesize profiles, score your catalog, and produce metrics.\n",
    "\n",
    "### Step 3: Do the blind evaluation\n",
    "Open the generated HTML, label each item, paste the JSON back.\n",
    "\n",
    "### Step 4: Analyze\n",
    "The analysis cells compute precision, recall, lift, and tier calibration automatically.\n",
    "\n",
    "### Tips for adaptation\n",
    "- **Different categories**: This works for any product category where visual browsing is the primary signal (furniture, shoes, jewelry, etc.)\n",
    "- **Fewer agents**: 3 agents is usually sufficient; 5 gives marginal improvement\n",
    "- **Model choice**: Sonnet is the best cost/quality tradeoff for scoring; use Opus for profile synthesis if budget allows\n",
    "- **Smaller catalogs**: Even 20-30 test items give meaningful signal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}